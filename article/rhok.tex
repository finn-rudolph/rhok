\documentclass[a4paper, 11pt, ngerman]{article}

\usepackage[algoruled, nosemicolon]{algorithm2e}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage[ngerman]{babel}
\usepackage[backend=biber, style=apa]{biblatex}
\usepackage{caption}
\usepackage{csquotes}
\usepackage[left=2.5cm, right=2.5cm, top=2.5cm, bottom=2cm]{geometry}
\usepackage[hidelinks]{hyperref}
\usepackage{mathtools}
\usepackage{pgfplots}
\usepackage{sectsty}
\usepackage[onehalfspacing]{setspace}

\title{Parametrisierung von Pollards Rho-Methode}
\author{Finn Rudolph}
\date{04.03.2024}

\addbibresource{rhok.bib}

\renewcommand{\thealgocf}{}

\newcommand{\C}{\mathbb{C}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Z}{\mathbb{Z}}
\renewcommand{\P}{\mathbb{P}}

\theoremstyle{definition}
\newtheorem{definition}{Definition}

\theoremstyle{plain}
\newtheorem{theorem}{Satz}
\newtheorem{lemma}{Lemma}

\theoremstyle{remark}
\newtheorem*{remark*}{Anmerkung}
\newtheorem*{assumption*}{Annahme}

\newcommand*\uparagraph{
        \par
        \nopagebreak
        \vskip3.25ex plus1ex minus.2ex
        \noindent
}

\sectionfont{\large}
\subsectionfont{\normalsize}

\begin{document}

\begin{titlepage}

    \noindent\rule{\textwidth}{0.4pt}

    \makeatletter
    \begin{flushleft}
        \textbf{\LARGE{\@title}} \\
        \vspace{1.5em}
        \@author \\
        \@date \\
        \vspace{1em}
        Erarbeitungsort: Hauptstraße 28, 96178 Pommersfelden \\
        Fachgebiet: Mathematik / Informatik \\
        Wettbewerbssparte: Jugend forscht \\
        Bundesland: Bayern \\
        Wettbewerbsjahr: 2024
    \end{flushleft}

    \vspace{0.2em}

    \section*{Projektüberblick}

    Pollards Rho-Methode ist einer der schnellsten Algorithmen zur Faktorisierung kleiner Zahlen. Bei der Implementierung des Algorithmus kann ein Parameter $k$ gewählt werden, der unter Umständen großen Einfluss auf die Laufzeit des Algorithmus hat, sowohl im positiven als auch im negativen Sinn. In dieser Arbeit soll untersucht werden, wie $k$ bestmöglich gewählt wird. Insbesondere ist der Fall interessant, wenn der Algorithmus auf mehreren Maschinen parallel ausgeführt wird, weil dann für jede Maschine $k$ separat gewählt werden kann. Für den Fall einer und zweier Maschinen konnten theoretische Ergebnisse erzielt werden, im Fall zweier Maschinen bleiben aber noch Fragen offen. Diese Ergebnisse decken sich mit durchgeführten Experimenten. Offen bleibt auch die Frage der optimalen Parametrisierung für drei oder mehr Maschinen.

    \vspace{0.5em}

    \begin{spacing}{1.2}
        \tableofcontents
    \end{spacing}

    \thispagestyle{empty}

\end{titlepage}

\newpage

\section{Zusammenfassung}

In dieser Arbeit wird die Frage behandelt, wie der Parameter $k$ in Pollards Rho-Methode optimal gewählt werden kann. Ein größerer Wert von $k$ erhöht grundsätzlich die Laufzeit, kann aber auch zu einer deutlichen Verringerung führen, wenn die Primfaktoren der zu faktorisierenden Zahl günstige Eigenschaften haben. Im Allgemeinen ist über die Primfaktoren allerdings nichts bekannt, denn sie sollen durch den Algorithmus bestimmt werden. Daher stellt sich die Frage, welcher Wert von $k$ im Mittel am besten ist. Es ergibt sich insbesondere dann ein interessantes Problem, wenn der Algorithmus auf mehreren Maschinen parallelisiert wird, weil $k$ dann für jede Maschine gewählt werden muss. Unter weithin anerkannten Annahmen über Pollards Rho-Algorithmus konnten grundlegende Methoden zur Beantwortung dieser Frage entwickelt werden. Mit diesen war es möglich zu zeigen, dass $k = 1$ für eine Maschine optimal ist. Im Fall zweier Maschinen wird gezeigt, dass $k_1 = k_2 = 1$ besser ist als wenn $k_1$ und $k_2$ Primzahlen sind oder wenn $k_1 = k_2 > 1$ gilt. $k_i$ bezeichnet den Wert von $k$ für die $i$-te Maschine. Anschließend werden Laufzeitmessungen vorgestellt, die die theoretischen Ergebnisse bestätigen.

\section{Motivation und Fragestellung}

Pollards Rho-Methode bleibt trotz der Existenz asymptotisch schnellerer Algorithmen einer der meist verwendeten Algorithmen zur Faktorisierung kleiner Zahlen. Gleichzeitig werden Leistungssteigerungen bei modernen Computern häufig durch größere Nebenläufigkeit (z. B. mehr Prozessorkerne) erzielt. Um die Rho-Methode schnellstmöglich zu implementieren, ist es also nötig zu untersuchen, wie sie am besten parallel ausgeführt werden kann. Bevor die Fragestellung präzise formuliert werden kann, soll jedoch Pollards Rho-Methode vorgestellt werden.

\subsection{Pollards Rho-Methode}
\label{sec:pollards-rho-method}

Sei $n$ die zu faktorisierende Zahl. Es wird angenommen, dass $n$ ungerade und keine Potenz einer natürlichen Zahl ist, da sonst einfach ein Faktor gefunden werden kann. Sei $h : \Z/n\Z \to \Z/n\Z$ mit $h : x \mapsto x^{2k} + 1$ für einen Parameter $1 \le k \in \N$. Man wähle einen zufälligen Anfangswert $x_0 \in \Z/n\Z$ und betrachte die Folge $(x_i)_{i \in \N}$, definiert durch $x_i = h(x_{i - 1})$. Da $(x_i)_{i \in \N}$ über der endlichen Menge $\Z/n\Z$ definiert ist, ist die Folge ab einem bestimmten Punkt periodisch. Sei $p$ ein Primfaktor von $n$ und $\pi : \Z/n\Z \to \Z/p\Z$ die natürliche Projektion. Die Idee der Rho-Methode ist, zwei Folgenglieder $x_i, x_j \in \Z/n\Z$ zu finden, sodass $x_i \ne x_j$ aber $\pi(x_i) = \pi(x_j)$. Dann ist nämlich $\gcd(n, x_i - x_j)$ ein echter Faktor von $n$. Das Ereignis, dass eine Folge einen Wert zweimal annimmt, wird eine \emph{Kollision} in dieser Folge genannt. Nimmt man an, dass die Periodenlänge von $(x_i)_{i \in \N}$ in $\Z/n\Z$ deutlich länger als die Periodenlänge in $\Z/p\Z$ ist, reicht es aus, $x_i, x_j$ mit $i \ne j$ zu finden, die kongruent modulo $p$ sind. Die Annahme ist plausibel, weil für den kleinsten Primfaktor $p \le \sqrt n$ gilt, es also deutlich weniger mögliche Werte für $\pi(x_i)$ als $x_i$ gibt. Im Folgenden ist $p$ immer der kleinste Primfaktor von $n$. Außerdem wird angenommen, dass die anderen Primfaktoren von $n$ so viel größer als $p$ sind, dass die Wahrscheinlichkeit einer Kollision modulo eines anderen Primfaktors vernachlässigbar gering ist. Zum Finden solcher $x_i, x_j$ ist es hilfreich, den funktionalen Graphen von $h$ zu betrachten.

\begin{definition}[Funktionaler Graph]
    Sei $X$ eine endliche Menge und $f: X \to X$ eine Abbildung. Der funktionale Graph von $f$, geschrieben $\gamma(f)$, ist der gerichtete Graph mit Knotenmenge $X$ und Kantenmenge $E$, wobei die Kante $(x, y) \in X \times X$ genau dann in $E$ liegt, wenn $f(x) = y$.
\end{definition}

\noindent Es ist leicht zu zeigen, dass jede Zusammenhangskomponente eines funktionalen Graphen aus einem Zyklus und an den Zyklusknoten gewurzelten Bäumen besteht. In $\gamma(h)$ ist $x_0$ also ein Knoten in einem der Bäume, der durch seinen Pfad zur Wurzel mit dem Zyklus seiner Zusammenhangskomponente verbunden ist. Die Folge $(x_i)_{i \in \N}$ startet bei $x_0$ und "`läuft"' durch den Graphen, wobei immer die eindeutige von einem Knoten ausgehende Kante entlanggegangen wird. An der Wurzel des Baums von $x_0$ wird der Zyklus in der Zusammenhangskomponente von $x_0$ betreten, und ab genau diesem Punkt ist $(x_i)_{i \in \N}$ periodisch. Bei Auftritt der ersten Kollision bildet der von $(x_i)_{i \in \N}$ abgelaufene Pfad die Form eines "`$\rho$"' in $\gamma(h)$.

Sei $f$ die Abbildung $h$, betrachtet in $\Z/p\Z$. Die Folge $(\pi(x_{i}))_{i \in \N}$ "`läuft"' also durch $\gamma(f)$, beginnend von $\pi(x_0)$. Unser Ziel, das Finden einer Kollision von $(\pi(x_i))_{i \in \N}$, ist also äquivalent dazu, einen Knoten in $\gamma(f)$ zu finden, der zweimal in dem von $(\pi(x_i))_{i \in \N}$ abgelaufenen Pfad erscheint. Dafür kann Floyds Algorithmus zur Zykluserkennung in $\gamma(f)$ verwendet werden. Floyds Algorithmus macht sich zunutze, dass es ein $1 \le r \in \N$ mit $\pi(x_r) = \pi(x_{2r})$ geben muss (\cite{knu98}, S. 7). Sei $\mu(f, \pi(x_0))$ die Höhe von $\pi(x_0)$ in seinem Baum und $\lambda(f, \pi(x_0))$ die Länge des Zyklus von $\pi(x_0)$. Für das minimale solcher $r$ gilt dann $r \le \mu(f, \pi(x_0)) + \lambda(f, \pi(x_0))$ (\cite{knu98}, S. 7). Wir nennen $\nu(f, \pi(x_0)) = \mu(f, \pi(x_0)) + \lambda(f, \pi(x_0))$ die Rho-Länge von $\pi(x_0)$ in $f$. Es würde also genügen, die Folgen $(\pi(x_i))_{i \in \N}$ und $(\pi(x_{2i}))_{n \in \N}$ gleichzeitig Glied für Glied zu berechnen und in jedem Schritt zu überprüfen, ob $\pi(x_i) = \pi(x_{2i})$. Das kann aber nicht explizit geschehen, da $p$ unbekannt ist. Stattdessen werden $(x_i)_{i \in \N}$ und $(x_{2i})_{i \in \N}$ Glied für Glied berechnet. Das Überprüfen, ob $\pi(x_i) = \pi(x_{2i})$ geschieht durch Berechnung von $\gcd(n, x_i - x_{2i})$. So erhält man nach maximal $\nu(f, \pi(x_0))$ Schritten die gewünschte Kollision. Die Methode kann wie folgt zusammengefasst werden.

\begin{algorithm*}
    $x \gets $ zufällige natürliche Zahl zwischen $0$ und $n - 1$ \;
    $y \gets x$ \;
    \While{\emph{\textsc{true}}}
    {
        $x \gets x^{2k} + 1 \mod n$ \;
        $y \gets (y^{2k} + 1)^{2k} + 1 \mod n$ \;
        $g \gets \gcd(n, x - y)$ \;
        \If{$g \ne 1 \text{\emph{\textbf{ and }}} g \ne n$}
        {
            \Return{$g$} \;
        }
    }

    \caption{Pollards Rho-Methode}
\end{algorithm*}

\noindent Die Analyse von Pollards Rho-Algorithmus erweist sich als schwierig, es ist bis dato keine rigorose Laufzeitanalyse bekannt. Unter heuristischen Annahmen lässt sich die Laufzeit allerdings gut abschätzen. Als erste Vereinfachung wird statt der mittleren Anzahl an Iterationen von Floyds Algorithmus die mittlere Rho-Länge analysiert. Eine zentrale Annahme dreht sich um die Verteilung der Rho-Längen in $\gamma(f)$, für deren Formulierung der Begriff einer asymptotischen Näherung benötigt wird.

\begin{definition}[Asymptotische Näherung]
    Eine Funktion $f : \R \to \R$ heißt genau dann asymptotische Näherung von einer Funktion $g : \R \to \R$, oder asymptotisch zu $g$, wenn
    \begin{align*}
        \lim_{x \to \infty} \frac {f(x)} {g(x)} = 1
    \end{align*}
    In diesem Fall schreiben wir $f \sim g$.
\end{definition}

\noindent Sei $A(n)$ die Menge der Abbildungen $\Z/n\Z \to \Z/n\Z$ für $n \in \N$. Über die Verteilung der Rho-Längen wird folgende Annahme getroffen, die auch \emph{Random Mapping Assumption} (RMA) genannt wird.

\begin{assumption*}[Random Mapping Assumption]
    Sei $f: \Z/p\Z \to \Z/p\Z$ mit $f : x \mapsto x^{2k} + 1$ und $d = \gcd(p - 1, 2k)$. Seien $x_0 \in \Z/p\Z$ und $y_0 \in \Z/((p - 1)/d)\Z$ zufällig und $g \in A((p - 1)/d)$ zufällig. Dann gilt $\P(\nu(f, x_0) = m) \sim \P(\nu(g, y_0) = m)$ für $p \to \infty$.
\end{assumption*}

\noindent In anderen Worten sagt die Random Mapping Assumption, dass sich die Verteilung der Rho-Längen von $f : x \mapsto x^{2k} + 1$ wie bei einer zufälligen Funktion aus $A((p - 1)/d)$ verhält. Insbesondere verhält sich $x \mapsto x^2 + 1$ bezüglich der Rho-Längen wie ein zufällige Funktion $\Z/(p - 1)\Z \to \Z/(p - 1)\Z$. \cite{bp81} geben eine Begründung für RMA. Im Folgenden wird statt $(p - 1)/d$ einfach $p/d$ verwendet, da $p \sim p - 1$ für $p \to \infty$.

Für $k = 1$ ist unter RMA die erwartete Anzahl an Iterationen der while-Schleife asymptotisch zu $\sqrt{\pi p / 2}$ (\cite{knu98}, S. 8). Da die Berechnung des größten gemeinsamen Teilers $O(\ln n)$ Schritte benötigt, ist die erwartete Laufzeit des Algorithmus $O(\sqrt p \ln n)$. Durch eine einfache Modifikation kann die Dauer des $\gcd$ amortisiert werden, sodass sich die Laufzeit auf $O(\sqrt p)$ verringert (\cite{bre80}). Damit ist pro Iteration also nur noch die Zeit zur Berechnung der $2k$-ten Potenzen von $x$ und $y$ relevant, was durchschnittlich in $c \lg 2k$ Schritten möglich ist, wobei $c$ eine hier unwichtige Konstante ist. Mit $\lg x$ wird der Logarithmus zur Basis 2 bezeichnet.

\subsection{Parallelisierung der Rho-Methode}

Sei $M$ die Anzahl verfügbarer Maschinen. Eine \emph{Maschine} meint hier nicht zwingend einen Computer, sondern eine Ressource, auf der ein sequentielles Programm ausgeführt werden kann, was beispielsweise auch ein Prozessorthread sein kann. Die Rho-Methode lässt sich parallelisieren, indem $M$ Anfangswerte zufällig und unabhängig voneinander gewählt werden und der Algorithmus auf jeder der $M$ Maschinen ausgeführt wird, bis eine der Maschinen einen Faktor findet. Nun ergibt sich folgende Frage, die in dieser Arbeit behandelt werden soll: \emph{Wie wählt man den Parameter $k$ für jede Maschine optimal, um eine möglichst geringe Laufzeit zu erzielen?} Das ist nicht sofort klar, da durch ein größeres $k$ möglicherweise $\gcd(p - 1, 2k)$ groß ist, sodass die Zahl an Iterationen um einen Faktor $\sqrt{\gcd(p - 1, 2k) -1}$ sinkt. Allerdings steigt die Dauer einer Iteration um einen Faktor $\lg 2k$. Da es sich bei den Veränderungen in der Laufzeit durch Veränderung von $k$ um konstante Faktoren handelt, wird für den Vergleich der Laufzeit nicht die $O$-Notation verwendet, sondern eine asymptotische Näherung für die erwartete Zahl an Zeiteinheiten bestimmt, wenn $p \to \infty$. Wir definieren eine \emph{Zeiteinheit} als die Dauer einer Iteration für $k = 1$, bei einer Maschine mit Parameter $k$ dauert eine Iteration also $\lg 2k$ Zeiteinheiten. Im Gegensatz zur $O$-Notation kann zwischen zwei Funktionen, die asymptotisch zueinander sind, für große $n$ kein konstanter Faktor liegen, sodass sich Veränderungen um konstante Faktoren sinnvoll vergleichen lassen.

Sei $k_1, \dots, k_M, 1 \le k_i \in \N$ eine Zuordnung von $k$-Werten für $M$ Maschinen. Mit $L_{k_1, \dots, k_M}(p)$ (oder kurz $L_{(k_i)}(p)$) wird die erwartete Laufzeit des parallelen Rho-Algorithmus mit entsprechenden $k$-Werten bezeichnet. Sei $h_i = p/(\gcd(p - 1, 2k_i) - 1)$. Unter RMA gilt
\begin{align}
    L_{(k_i)}(p) = \E \bigg ( \min_{i = 1}^M Z_i \lg 2k_i \bigg )
    \label{eq:lki-definition}
\end{align}
wobei $Z_i$ die gleichverteilte Zufallsvariable über $A(h_i) \times \Z/h_i\Z$ ist, mit $Z_i(f, x_0) = \nu(f, x_0)$ für $f \in A(h_i), x_0 \in \Z/h_i\Z$. In $L_{(k_i)}(p)$ ist $\gcd(p - 1, 2k_i)$ für alle $1 \le i \le M$ noch fixiert, der Erwartungswert über alle Möglichkeiten von $\gcd(p - 1, 2k_i)$ wird erst in Abschnitt \ref{sec:optimal-k} behandelt.

Wenn $k_i = k_j$, sind $Z_i$ und $Z_j$ nicht stochastisch unabhängig, weil sich die $i$-te und $j$-te Maschine dann im gleichen funktionalen Graphen bewegen. Es kann beispielsweise sein, dass die $i$-te Maschine der $j$-ten "hinterherläuft", wenn der Anfangswert der $i$-ten in einem Teilbaum des Anfangswerts der $j$-ten liegt. Diese Abhängigkeit sorgt für eine schlechtere Laufzeit und erschwert die Analyse des Algorithmus. Das Problem kann umgangen werden, indem die Funktion $f(x) = x^{2k_i} + c_i$ statt $f(x) = x^{2k_i} + 1$ für Maschine $i$ verwendet wird (\cite{cr99}, S. 6). Dabei wird $c_i \ne 0, -2$ für jede Maschine unabhängig gewählt, sodass $Z_i$ und $Z_j \ (i \ne j)$ immer stochastisch unabhängig sind, auch wenn $k_i = k_j$. Für folgende Berechnungen wird daher immer angenommen, dass $Z_i$ und $Z_j$ für $i \ne j$ unabhängig sind.

\begin{remark*}
    In der Version dieser Arbeit zum Regionalwettbewerb war mir das Paper von \cite{cr99} noch nicht bekannt. Daher war mir nicht klar, dass sich die Abhängigkeit von Maschinen mit gleichem $k$ durch unabhängige Wahl von $c_i$ umgehen lässt, sondern es wurde nur $c_i = 1$ für alle $i$ in Betracht gezogen. Folglich wurde der Fall unabhängiger Maschinen ($\Longleftrightarrow$ paarweise verschiedene $k_i$) vom Fall abhängiger Maschinen unterschieden. Für den Fall unabhängiger Maschinen wurde die Formel aus Abschnitt \ref{sec:formula-running-time} hergeleitet, die durch zufällige Wahl von $c_i$ nun allgemein gilt. Im Fall abhängiger Maschinen konnte eine Formel für $M = 2$ bestimmt werden. Sie ist hier für weitere Berechnungen nicht mehr relevant. Weil der zentrale Satz in der Herleitung auch unabhängig von der Anwendung auf die Rho-Methode interessant ist, soll dieser vorgestellt werden. Der Beweis wird aufgrund seiner Länge weggelassen. Im Fall zweier abhängiger Maschinen bewegen sich beide Maschinen im gleichen funktionalen Graphen, beginnend von unabhängig gewählten Anfangsknoten. Die erwartete Anzahl an Iterationen ist also der Erwartungswert des Minimums der beiden Rho-Längen der Anfangsknoten. Diese Zahl wird unter RMA von folgendem Satz beschrieben.
\end{remark*}

\begin{theorem}
    \label{theorem:min-rho-len-m2}
    Sei $n \in \N$ und $A(n)$ die Menge der Abbildungen $\Z/n\Z \to \Z/n\Z$. Wir bezeichnen mit
    \begin{align*}
        \tau_n =  \frac 1 {n^{n + 2}}
        \sum_{g \in A(n)} \, \sum_{a \in \Z/n\Z} \, \sum_{b \in \Z/n\Z}
        \min\{\nu(g, a), \nu(g, b)\}
    \end{align*}
    die erwartete minimale Rho-Länge zweier zufälliger Knoten in einem zufälligen funktionalen Graphen von Größe $n$.  Es gilt
    \begin{align*}
        \tau_n \sim \frac {25} {32} \sqrt{\pi n / 2}
    \end{align*}
\end{theorem}

Der Vorfaktor in der Definition von $\tau_n$ ist $1/n^{n + 2}$, da es $n^n$ Abbildungen $\Z/n\Z \to \Z/n\Z$ gibt und für jede von diesen $n^2$ Paare an Anfangswerten.

\section{Herleitung einer Formel für die erwartete Laufzeit}
\label{sec:formula-running-time}

In diesem Abschnitt wird eine Formel für $L_{(k_i)}(p)$ hergeleitet. Sei $h_i = p/(\gcd(p - 1, 2k_i) - 1)$. Mit (\ref{eq:lki-definition}) und der Definition des Erwartungswerts gilt
\begin{align}
    L_{(k_i)}(p) =
    \sum_{z_1 = 1}^{h_1} \P(Z_1 = z_1)
    \sum_{z_2 = 1}^{h_2} \P(Z_2 = z_2) \, \cdots
    \sum_{z_M = 1}^{h_M} \P(Z_M = z_M)
    \, \min_{i = 1}^M(z_i \lg 2k_i)
    \label{eq:lki-written-out}
\end{align}
wobei durch "`$\cdots$"' $M$ ineinander verschachtelte Summen über alle möglichen $z_i$ für jedes $1 \le i \le M$ angedeutet werden. Zunächst soll $\P(Z_i = z_i)$ bestimmt werden.

\begin{lemma}
    Man betrachte eine Maschine mit Parameter $k$ und $h = p/(\gcd(p - 1, 2k) - 1)$. Sei $Z$ die gleichverteilte Zufallsvariable über $A(h) \times \Z/h\Z$ mit $Z(f, x_0) = \nu(f, x_0)$. Es gilt
    \begin{align*}
        \P(Z = z) = \frac z h \prod_{j = 0}^{z - 1} \bigg (1 - \frac j h \bigg )
    \end{align*}

    \label{lemma:prob-s-z}
\end{lemma}

\begin{proof}
    Für eine zufällige Funktion $f \in A(h)$ ist die Wahrscheinlichkeit einer Kollision im $j$-ten Schritt $j/h$, wenn in den ersten $j - 1$ Schritten keine Kollision aufgetreten ist, da jeder der $h$ möglichen Werte gleich wahrscheinlich ist und $j$ von ihnen zu einer Kollision führen. Es gilt also
    \begin{align*}
        \P(Z = z)
        = \frac z h \prod_{j = 0}^{z - 1} \bigg (1 - \frac j h \bigg )
    \end{align*}
    da in den ersten $z - 1$ Schritten keine Kollision auftreten darf und im $z$-ten Schritt eine Kollision auftreten muss.
\end{proof}

Sei im Folgenden $Q(z, h) = (z/h) e^{-z^2/(2h)}$ und
\begin{align*}
    F =
    \sum_{z_1 = 1}^{h_1} Q(z_1, h_1)
    \sum_{z_2 = 1}^{h_2} Q(z_2, h_2) \, \cdots
    \sum_{z_M = 1}^{h_M} Q(z_M, h_M)
    \, \min_{i = 1}^M(z_i \lg 2k_i)
\end{align*}
Das Ziel ist nun, eine einfache asymptotische Näherung für $F$ zu finden und zu zeigen, dass $L_{(k_i)}(p) \sim F$.


\begin{lemma}
    Sei $0 \le \delta < 1/6$. Mit der Notation von Lemma \ref{lemma:prob-s-z} gilt für $p \to \infty$ und $z \le h^{1/2 + \delta}$
    \begin{align*}
        |Q(z, h) - \P(Z = z)| = O \bigg ( \frac 1 {h^{1 - 4\delta}} \bigg )
    \end{align*}

    \label{lemma:prob-s-z-asmyp}
\end{lemma}

\begin{remark*}
    $z$ wird hier als Funktion von $h$ verstanden. Denn um (\ref{eq:lki-written-out}) später für $p \to \infty$ auszuwerten, reicht eine Abschätzung von $\P(Z_i = z_i)$ für konstante $z_i$ nicht aus, da über alle $1 \le z_i \le h_i$ summiert wird und $h_i = p/(\gcd(p - 1, 2k_i) - 1)$.

    Aus Lemma \ref{lemma:prob-s-z-asmyp} folgt $\lim_{p \to \infty} \P(Z = z) = Q(z, h)$ für $z \le h^{1/2 + \delta}$. Mit einer einfachen Rechnung lässt sich das auch für $z > h^{1/2 +\delta}$ zeigen. Daraus folgt aber noch nicht $L_{(k_i)}(p) \sim F$, da die Anzahl an Summanden in (\ref{eq:lki-written-out}) von $p$ abhängt. Das $\delta$ in Lemma \ref{lemma:prob-s-z-asmyp} wird später hilfreich sein, um zu zeigen, dass wirklich die gesamte Summe in (\ref{eq:lki-written-out}) asymptotisch zu $F$ ist.
\end{remark*}

\begin{proof}[Beweis von Lemma \ref{lemma:prob-s-z-asmyp}]
    Zunächst wird die Restgliedabschätzung $e^x = 1 + x + O(x^2)$ für $|x| \le 1$ auf $\P(Z = z)$ angewandt. Damit gilt
    \begin{align*}
        \P(Z = z) = \frac z h \prod_{j = 0}^{z - 1}
        \Bigg ( e^{-j/h} - O \bigg (\frac {j^2} {h^2} \bigg ) \Bigg )
    \end{align*}
    Daraus folgt
    \begin{align*}
        \vert Q(z, h) - \P(Z = z) \vert
         & = \frac z h \Bigg \vert
        e^{-z^2/(2h)} -
        \prod_{j = 0}^{z - 1}
        \Bigg ( e^{-j/h} - O \bigg (\frac {j^2} {h^2} \bigg ) \Bigg )
        \Bigg \vert                      \\
         & \le \frac z h \Bigg \vert
        e^{-z^2/(2h)} -
        \prod_{j = 0}^{z - 1} e^{-j/h}
        \Bigg \vert                      \\
         & \quad + \frac z h \Bigg \vert
        \sum_{k = 0}^{z - 1} O \bigg ( \frac {k^2} {h^2} \bigg )
        \prod_{j = 0, j \ne k}^{z - 1} e^{-j/h}
        - \sum_{k = 0}^{z - 1}\sum_{l = k+1}^{z - 1}
        O \bigg ( \frac {k^2l^2} {h^4} \bigg )
        \prod_{j = 0, j \ne k,l}^{z - 1} e^{-j/h}
        + \cdots
        \Bigg \vert
    \end{align*}
    Die Terme des ausmultiplizierten Produkts wurden nach der Zahl an $O(j^2/h^2)$-Faktoren gruppiert. Außerdem wurde die Dreiecksungleichung angewandt. Für den ersten Summanden gilt
    \begin{align*}
        \frac z h \Bigg \vert
        e^{-z^2/(2h)} -
        \prod_{j = 0}^{z - 1} e^{-j/h}
        \Bigg \vert
         & = \frac z h \Big \vert
        e^{-z^2/(2h)} -
        e^{-\sum_{j = 0}^{z - 1} j/h}
        \Big \vert                                       \\
         & = \frac z h \Big \vert
        e^{-z^2/(2h)} -  e^{-z(z - 1)/(2h)}
        \Big \vert                                       \\
         & = \frac z h e^{-z^2/(2h)} \Big \vert
        1 - e^{z/(2h)}
        \Big \vert                                       \\
         & \le \frac {z^2} {h^2}                         \\
         & = O \bigg ( \frac 1 {h^{1 - 2\delta}} \bigg )
    \end{align*}
    Von der dritten zur vierten Zeile wurde erneut die Restgliedabschätzung verwendet. Für den zweiten Summanden gilt
    \begin{align*}
               & \frac z h \Bigg \vert
        \sum_{k = 0}^{z - 1} O \bigg ( \frac {k^2} {h^2} \bigg )
        \prod_{j = 0, j \ne k}^{z - 1} e^{-j/h}
        - \sum_{k = 0}^{z - 1}\sum_{l = k+1}^{z - 1}
        O \bigg ( \frac {k^2l^2} {h^4} \bigg )
        \prod_{j = 0, j \ne k,l}^{z - 1} e^{-j/h}
        + \cdots
        \Bigg \vert                                           \\
        \le \, & \frac z h \Bigg (
        \sum_{k = 0}^{z - 1} O \bigg ( \frac {k^2} {h^2} \bigg )
        \prod_{j = 0, j \ne k}^{z - 1} e^{-j/h}
        + \sum_{k = 0}^{z - 1}\sum_{l = k+1}^{z - 1}
        O \bigg ( \frac {k^2l^2} {h^4} \bigg )
        \prod_{j = 0, j \ne k,l}^{z - 1} e^{-j/h}
        + \cdots \Bigg )                                      \\
        \le \, & \frac z h \Bigg (
        \sum_{k = 0}^{z - 1} O \bigg ( \frac {k^2} {h^2} \bigg )
        + \sum_{k = 0}^{z - 1}\sum_{l = k+1}^{z - 1}
        O \bigg ( \frac {k^2l^2} {h^4} \bigg )
        + \sum_{k = 0}^{z - 1}\sum_{l = k+1}^{z - 1}\sum_{m = l + 1}^{z - 1}
        O \bigg ( \frac {k^2l^2m^2} {h^6} \bigg )
        + \cdots \Bigg )                                      \\
        \le \, & \frac z h \Bigg (
        z \, O \bigg ( \frac {z^2} {h^2} \bigg )
        + z^2 \, O \bigg ( \frac {z^4} {h^4} \bigg )
        + z^3 \, O \bigg ( \frac {z^6} {h^6} \bigg )
        + \cdots \Bigg )                                      \\
        = \,   & O \Bigg ( \frac 1 {h^{1/2 - \delta}} \bigg (
        \frac 1 {h^{1/2 - 3 \delta}} + \frac 1 {h^{1 - 6\delta}}
        + \frac 1 {h^{3/2 - 9\delta}} + \cdots
        \bigg ) \Bigg )                                       \\
        = \,   & O \bigg ( \frac 1 {h^{1 - 4\delta}} \bigg )
    \end{align*}
    Von der ersten zur zweiten Zeile wurden alle negativen Vorzeichen entfernt. Anschließend wurden die Produkte von $e^{-j/h}$ weggelassen, da sie $\le 1$ sind. In der letzten Zeile wurde die geometrische Summenformel verwendet.
\end{proof}

\begin{lemma}
    Sei $F$ wie oben definiert. Es gilt
    \begin{align*}
        F \sim \sqrt{\pi p / 2} \Bigg ( \sum_{i = 1}^M \frac {\gcd(p - 1, 2k_i) - 1} {\lg^2 2k_i} \Bigg )^{-1/2}
    \end{align*}

    \label{lemma:f-asmyp}
\end{lemma}

\begin{proof}
    Die Summen werden durch Integrale angenähert.
    \begin{align*}
        F \sim
        \int_0^{h_1} Q(z_1, h_1)
        \int_0^{h_2} Q(z_2, h_2) \cdots
        \int_0^{h_M} Q(z_M, h_M)
        \min_{i = 1}^M(z_i \lg 2k_i) \, dz_M \cdots dz_2 dz_1
    \end{align*}
    Da
    \begin{align*}
        \int_0^{h_2} Q(z_2, h_2) \cdots \int_0^{h_M} Q(z_M, h_M)
        \min_{i = 1}^M(z_i \lg 2k_i) \, dz_M \cdots dz_2 \le p^M
    \end{align*}
    gilt
    \begin{align*}
            & \int_{h_1}^{\infty} Q(z_1, h_1)
        \int_0^{h_2} Q(z_2, h_2) \cdots
        \int_0^{h_M} Q(z_M, h_M)
        \min_{i = 1}^M(z_i \lg 2k_i) \, dz_M \cdots dz_2 dz_1 \\
        \le & \ p^M \int_{h_1}^{\infty}
        \frac {z_1} {h_1} e^{-z_1^2/(2h_1)} dz_1              \\
            & = p^M e^{-h_1/2} \to 0 \quad (p \to \infty)
    \end{align*}
    Daraus folgt
    \begin{align*}
        F \sim
        \int_0^{\infty}
        \frac {z_1} {h_1} e^{-z_1^2/(2h_1)} \cdots
        \int_0^{\infty}
        \frac {z_M} {h_M} e^{-z_M^2/(2h_M)}
        \min_{i = 1}^M(z_i \lg 2k_i) \, dz_M \cdots dz_1
    \end{align*}
    Nun wird ein Variablenwechsel $y_i = z_i \lg 2k_i$ durchgeführt. Damit gilt
    \begin{align*}
        F \sim
        \prod_{i = 1}^M \frac 1 {\lg 2k_i}
        \int_0^{\infty}
        \frac {y_1 e^{-y_1^2 / (2h_1 \lg^2 2k_1)}} {h_1\lg 2k_1}  \cdots
        \int_0^{\infty}
        \frac {y_M e^{-y_M^2 / (2h_M \lg^2 2k_M)}} {h_M \lg 2k_M}
        \min_{i = 1}^M(y_i) \, dy_M \cdots dy_1
    \end{align*}
    Anstatt über alle $y_i$ von $0$ bis $\infty$ zu integrieren, wird nun unterschieden, welches der $y_i$ das Minimum ist. Wenn $y_j$ das Minimum ist, wird über $y_j$ von $0$ bis $\infty$ integriert und über $y_i \ (i \ne j)$ von $y_i$ bis $\infty$. So kann $\min_{i = 1}^M (y_i)$ nach vorne gezogen werden, sodass die Integrale für $i \ne j$ unabhängig voneinander ausgewertet werden. Schließlich wird über alle möglichen $j$ aufsummiert.
    \begin{align*}
        F & \sim
        \prod_{i = 1}^M \frac 1 {\lg 2k_i}
        \sum_{j = 1}^M \int_0^\infty
        \frac {y_j^2 e^{-y_j^2 / (2h_j \lg^2 2k_j)}} {h_j \lg 2k_j}
        \prod_{i = 1, i \ne j}^M \int_{y_j}^{\infty}
        \frac {y_i e^{-y_i^2 / (2h_i \lg^2 2k_i)}} {h_i \lg 2k_i} dy_i \; dy_j
    \end{align*}
    Für eine einfachere Notation werden die Integrationsvariablen von $y_j$ zu $y$ und von $y_i$ zu $x$ umbenannt. Zunächst werden die Integrale im Produkt rechts ausgewertet. Es gilt
    \begin{align*}
        \int_{y}^{\infty}
        \frac {x e^{-x^2 / (2h_i \lg^2 2k_i)}} {h_i \lg 2k_i} dx
         & = e^{-x^2/(2h_i \lg^2 2k_i)} (- \lg 2k_i) \Big \vert_{y}^{\infty} \\
         & = e^{-y^2/(2h_i \lg^2 2k_i)} \lg 2k_i
    \end{align*}
    Also gilt
    \begin{align*}
        F & \sim
        \prod_{i = 1}^M \frac 1 {\lg 2k_i}
        \sum_{j = 1}^M \int_0^\infty
        \frac {y^2 e^{-y^2 / (2h_j \lg^2 2k_j)}} {h_j \lg 2k_j}
        \prod_{i = 1, i \ne j}^M e^{-y^2/(2h_i \lg^2 2k_i)} \lg 2k_i \; dy \\
          & = \prod_{i = 1}^M \frac 1 {\lg 2k_i}
        \sum_{j = 1}^M \int_0^\infty
        \frac {y^2} {h_j \lg^2 2k_j} \,
        \prod_{i = 1}^M e^{-y^2/(2h_i \lg^2 2k_i)} \lg 2k_i \; dy          \\
          & = \sum_{j = 1}^M \int_0^\infty
        \frac {y^2} {h_j \lg^2 2k_j} \,
        \prod_{i = 1}^M e^{-y^2/(2h_i \lg^2 2k_i)} \; dy                   \\
          & = \Bigg ( \sum_{i = 1}^M \frac 1 {h_i \lg^2 2k_i} \Bigg )
        \int_0^\infty
        y^2 \,e^{- \sum_{i = 1}^M y^2/(2h_i \lg^2 2k_i)} \; dy
    \end{align*}
    Um das Integral auszuwerten, wurde die Tabelle in Wikipedia: \cite{gint} verwendet. Damit erhält man schließlich
    \begin{align*}
        F & \sim \Bigg (\sum_{i = 1}^M \frac 1 {h_i \lg^2 2k_i} \Bigg ) \,
        \sqrt {\pi / 2} \
        \Bigg ( \sum_{i = 1}^M \frac 1 {h_i \lg^2 2k_i} \Bigg )^{-3/2}     \\
          & = \sqrt{\pi p / 2} \; \Bigg (
        \sum_{i = 1}^M \frac {\gcd(p - 1, 2k_i) - 1} {\lg^2 2k_i} \Bigg )^{-1/2}
    \end{align*}
\end{proof}

\begin{lemma}
    Sei $Q(z, h) = (z/h)e^{-z^2/(2h)}$ und $\delta > 0$. Dann gilt für $h \to \infty$
    \begin{align*}
        \sum_{z = h^{1/2 + \delta}}^h Q(z, h) = O\Big (e^{-h^{2\delta}} \Big )
    \end{align*}

    \label{lemma:q-large-z-asymp}
\end{lemma}

\begin{proof}
    Es gilt
    \begin{align*}
        \sum_{z = h^{1/2 + \delta}}^h \frac z h e^{-z^2/2h}
        \le \sum_{z = h^{1/2 + \delta}}^h e^{-h^{2\delta}}
        \le he^{-h^{2\delta}} = O \Big (e^{-h^{2\delta}} \Big )
    \end{align*}
\end{proof}

Um $L_{(k_i)}(p) \sim F$ zu zeigen, schreibe man
\begin{align*}
    F = & \sum_{z_1 = 1}^{h_1^{9/16}} Q(z_1, h_1)
    \sum_{z_2 = 1}^{h_2} Q(z_2, h_2)    \cdots
    \sum_{z_M = 1}^{h_M} Q(z_M, h_M)
    \min_{i = 1}^M (z_i \lg 2k_i)                 \\
        & \quad +
    \sum_{z_1 = h_1^{9/16}}^{h_1} Q(z_1, h_1)
    \sum_{z_2 = 1}^{h_2} Q(z_2, h_2)    \cdots
    \sum_{z_M = 1}^{h_M} Q(z_M, h_M)
    \min_{i = 1}^M (z_i \lg 2k_i)
\end{align*}
Die zweite Summe geht nach Lemma \ref{lemma:q-large-z-asymp} gegen 0. Indem auf die erste Summe Lemma \ref{lemma:prob-s-z-asmyp} mit $\delta = 1/16$ angewandt wird, folgt
\begin{align*}
    F \sim \sum_{z_1 = 1}^{h_1^{9/16}}
    \Big ( \P(Z_1 = z_1) + O \Big ( h_1^{-3/4} \Big ) \Big )
    \sum_{z_2 = 1}^{h_2} Q(z_2, h_2)    \cdots
    \sum_{z_M = 1}^{h_M} Q(z_M, h_M)
    \min_{i = 1}^M (z_i \lg 2k_i)
\end{align*}
Nach Lemma \ref{lemma:f-asmyp} gilt $\sum_{z_2 = 1}^{h_2} Q(z_2, h_2) \cdots \sum_{z_M = 1}^{h_M} Q(z_M, h_M) \min_{i = 1}^M (z_i \lg 2k_i) = O(\sqrt p)$. Folglich ist die Summe aller Terme mit einem $O \big (h_2^{-3/4} \big )$-Faktor durch $O(p^{5/16})$ beschränkt. Für eine asymptotische Näherung von $F$ können sie also weggelassen werden. Indem dieses Argument für $Q(z_2, h_2) \dots, Q(z_M, h_M)$ wiederholt wird, können alle $Q(z_i, h_i)$ durch $\P(Z_i = z_i)$ ersetzt werden. Durch eine erneute Anwendung von Lemma \ref{lemma:q-large-z-asymp} und $\P(z_i, h_i) = O(Q(z_i, h_i))$ können die oberen Grenzen der Summen von $h_i^{9/16}$ zurück zu $h_i$ geändert werden, sodass $L_{(k_i)}(p) \sim F$ folgt.

Aus Lemma \ref{lemma:f-asmyp} und $L_{(k_i)}(p) \sim F$ folgt nun eine asymptotische Näherung für die Laufzeit der Rho-Methode.
\begin{theorem}
    Unter der Random Mapping Assumption gilt für die erwartete Laufzeit der Rho-Methode auf $M$ Maschinen mit $k$-Werten $k_1, \dots, k_M$
    \begin{align}
        L_{k_1, \dots, k_M}(p) \sim
        \sqrt{\pi p / 2} \Bigg ( \sum_{i = 1}^M
        \frac {\gcd(p - 1, 2k_i) - 1} {\lg^2 2k_i} \Bigg )^{-1/2}
        \label{eq:lki-asymp}
    \end{align}

    \label{theorem:lki-asymp}
\end{theorem}

\section{Bestimmung optimaler Exponenten für die Rho-Methode}
\label{sec:optimal-k}

In diesem Abschnitt wird die Frage behandelt, wie der Parameter $k$ bei $M$ Maschinen bestmöglich gewählt werden kann. Mit Satz \ref{theorem:lki-asymp} konnten Ergebnisse in den Fällen $M = 1$ und $M = 2$ erzielt werden. Die grundlegende Strategie ist, den Erwartungswert von $L_{k_1, \dots, k_M}(p)$ über alle Möglichkeiten von $\gcd(p - 1, 2k_i)$ für alle $1 \le i \le M$ zu bilden und so einen Wert für die erwartete Laufzeit in Abhängigkeit der $k_i$ zu erhalten. Da $p - 1$ gerade ist, gilt $\gcd(p-1, 2k_i) = 2\gcd((p - 1)/2, k_i)$. Weitere Kongruenzen von $p - 1$ sind im Allgemeinen nicht bekannt, weshalb angenommen wird, dass jeder Rest von $(p- 1)/2$ modulo $k_i$ gleich wahrscheinlich ist.

\subsection{Der Fall einer Maschine}

\begin{theorem}
    \label{theorem:optimal-k-m1}
    Sei $L_k(p)$ wie in (\ref{eq:lki-definition}) definiert. Der Erwartungswert $\E(L_k(p))$ über alle möglichen $\gcd((p - 1)/2, k)$ nimmt ein globales Minimum für $k = 1$ an.
\end{theorem}

\begin{proof}
    Durch Einsetzen von $M = 1$ in (\ref{eq:lki-asymp}) erhalten wir
    \begin{align*}
        L_k(p) \sim \sqrt {\pi p / 2} \;
        \frac {\lg 2k} {\sqrt{\gcd(p - 1, 2k) - 1}}
    \end{align*}
    Die erwartete Laufzeit im Fall $k = 1$ ist folglich $\sqrt{\pi p/2}$. Es wird also gezeigt, dass $\E(L_k(p)) > \sqrt{\pi p / 2}$ für $k > 1$. Mit $\varphi$ wird die eulersche Phifunktion bezeichnet. Dann gilt
    \begin{align*}
        \E(L_k(p))
        \sim \sqrt{\pi p / 2} \, \lg (2k) \,
        \sum_{d | k} \frac {\P(\gcd((p - 1)/2, k) = d)}
        { \sqrt {2d - 1}} \nonumber
        \ge \sqrt{\pi p / 2} \, \lg (2k) \, \frac {\varphi(k)} k
    \end{align*}
    Für die Ungleichung wurde statt der Summe über alle Teiler nur $d = 1$ betrachtet. Da es $\varphi(k)$ teilerfremde Zahlen kleiner $k$ gibt, ist $\P(\gcd((p - 1)/2, k) = 1) = \varphi(k)/k$. Nach \cite{rs62}, Theorem 15 gilt $ \varphi(k) / k > 1 / (e^\gamma \ln (\ln (k)) + 2.51 / \ln (\ln (k)))$ für $k \ge 3$, wobei $\gamma \approx 0.5772$ die Euler-Mascheroni-Konstante ist. Für $k \ge e^e$ folgt daraus $\varphi(k) / k > 1/(e^\gamma \ln(\ln(k)) + 2.51)$. Indem nun gezeigt wird, dass $\lg (2k) / (e^\gamma \ln (\ln (k)) + 2.51) > 1$ für $k \ge 16$ wird der Satz im Fall $k \ge 16$ bewiesen. Sei $f(x) = \lg (2x) / (e^\gamma \ln (\ln (x)) + 2.51)$. Es gilt $f(16) \approx 1.1557$ und
    \begin{align*}
        f'(x)
        = \frac {\ln (x)(\ln (\ln (x)) + 1.51) - \ln 2}
        {x \ln (2) \ln (x)(\ln(\ln(x)) + 2.51)^2}
    \end{align*}
    Für $x \ge 16$ ist der Nenner von $f'$ positiv, denn $x > 0$ und $\ln x > 0$, und weil $\ln \ln x > 1$ für $x \ge 16$ ist der Term unter dem Quadrat positiv. Der Zähler ist ebenfalls positiv, da $\ln x \ge 1$ und $\ln \ln x \ge 1$, woraus $\ln(x)(\ln(\ln(x)) + 1.51) \ge 1 \cdot (1 + 1.51) = 2.51 > \ln 2$ folgt. Also ist $f$ streng monoton steigend für $x \ge 16$, und da bereits $f(16) > 1$ gezeigt wurde, folgt $f(x) > 1$ für $x \ge 16$. Der Fall $k < 16$ wurde durch Ausrechnen von (\ref{eq:lki-asymp}) für $2 \le k \le 15$ überprüft.
\end{proof}

\subsection{Der Fall zweier Maschinen}

\begin{theorem}
    \label{theorem:optimal-k-m2}
    Sei $L_{k_1, k_2}(p)$ wie in (\ref{eq:lki-definition}) definiert. Man nehme RMA an und bilde den Erwartungswert $\E(L_{k_1, k_2})$ über alle möglichen $\gcd((p - 1)/2, k_i)$ für $i = 1, 2$.
    \begin{enumerate}
        \item Wenn $k_1, k_2$ Primzahlen sind, gilt $\E(L_{1, 1}(p)) < \E(L_{k_1, k_2}(p))$.
        \item Wenn $1 < k \in \N$, gilt $\E(L_{1, 1}(p)) < \E(L_{k, k}(p))$.
    \end{enumerate}
\end{theorem}

\begin{proof}
    Nach Satz \ref{theorem:lki-asymp} gilt
    \begin{align*}
        L_{k_1, k_2}(p)
         & \sim \sqrt{\pi p / 2} \
        \Bigg ( \frac {\gcd(p - 1, 2k_1) - 1} {\lg^2 2k_1} +
        \frac {\gcd(p -1, 2k_2) - 1} {\lg^2 2k_2} \Bigg )^{-1/2}
    \end{align*}
    Durch Einsetzen von $k_1 = k_2 = 1$ erhalten wir $L_{1, 1}(p) \sim \sqrt{\pi p / 4}$. Es gilt also in beiden Teilen zu zeigen, dass der Erwartungswert größer ist. Zunächst wird Teil 1 bewiesen.

    Durch Bilden des Erwartungswerts über alle möglichen $\gcd((p - 1)/2, k_i)$ für $i = 1, 2$ erhält man
    \begin{align}
        \E(L_{k_1, k_2})
         & \sim \sqrt{\pi p / 2}
        \sum_{d_1 | k_1} \P(\gcd((p - 1)/2, k_1) = d_1)
        \nonumber                                                 \\
         & \qquad \sum_{d_2 | k_2} \P(\gcd((p - 1)/2, k_2) = d_2)
        \Bigg ( \frac {2d_1 - 1} {\lg^2 2k_1}
        + \frac {2d_2 - 1} {\lg^2 2k_2} \Bigg )^{-1/2}
        \nonumber                                                 \\
         & \ge \sqrt{\pi p / 2} \
        \frac {\varphi(k_1) \varphi(k_2)} {k_1k_2}
        \Bigg (\frac 1 {\lg^2 2k_1} + \frac 1 {\lg^2 2k_2} \Bigg )^{-1/2}
        \nonumber                                                 \\
         & = \sqrt{\pi p / 2} \
        \frac {(k_1 - 1) (k_2- 1)} {k_1k_2}
        \sqrt{\frac{\lg^2(2k_1) \lg^2(2k_2)}{\lg^2(2k_1) + \lg^2(2k_2)}}
        \label{eq:two-mach-lowerb}
    \end{align}
    Wie bei $M = 1$ wurden die Summen über alle Teiler von $k_1, k_2$ durch den Wert bei $d_1 = d_2 = 1$ nach unten begrenzt. Die Terme $(k_i - 1)/k_i$ sind streng monoton steigend in $k_i$. Ebenso ist das Argument der nachfolgenden Wurzel in (\ref{eq:two-mach-lowerb}) streng monoton steigend in jedem der $k_i$. Um das zu zeigen, sei $x = \lg^2 2k_1, y = \lg^2 2k_2$ und $x' > x$. Dann gilt
    \begin{align*}
        \frac {x'y} {x' + y}
        = \frac {x'y} {x' + y} \, \frac {x + y} {xy} \, \frac {xy} {x + y}
        = \frac {xx'y + x'y^2} {xx'y + xy^2} \, \frac {xy} {x + y}
        > \frac {xy} {x + y}
    \end{align*}
    da $x, x', y > 0$ und $x < x'$. Der Term ist symmetrisch in $x$ und $y$, womit er auch streng monoton steigend in $y$ ist. Da die Wurzelfunktion streng monoton steigt und die Verkettung streng monoton steigender Funktionen streng monoton steigt, folgt, dass die gesamte Wurzel auf der rechten Seite von (\ref{eq:two-mach-lowerb}) streng monoton steigt. Weil nun jeder einzelne Faktor in (\ref{eq:two-mach-lowerb}) streng monoton steigt und positiv ist, ist ganz (\ref{eq:two-mach-lowerb}) streng monoton steigend in den $k_i$. Indem man $k_1 = k_2 = 3$ in (\ref{eq:two-mach-lowerb}) einsetzt, sieht man, dass $E(L_{3, 3}(p)) \ge 0.8123 \sqrt{\pi p /2}  > \sqrt{\pi p / 4}$. Weil (\ref{eq:two-mach-lowerb}) symmetrisch in $k_1, k_2$ ist, kann $k_1 < k_2$ angenommen werden. Dann folgt aus der Monotonie von (\ref{eq:two-mach-lowerb}), dass $E(L_{k_1, k_2}(p)) > \E(L_{1, 1}(p))$, wenn $k_1 \ge 3$. Es bleibt also lediglich der Fall $k_1 = 2$. Durch Einsetzen von $k_1 = 2, \; k_2 = 7$ in (\ref{eq:two-mach-lowerb}) gilt $\E(L_{2, 7}(p)) \ge 0.7588 \sqrt{\pi p / 2} > \sqrt{\pi p / 4}$. Aus der Monotonie von (\ref{eq:two-mach-lowerb}) folgt $\E(L_{k_1, k_2}(p)) > \E(L_{1, 1}(p))$ für $k_1 = 2$ und $k_2 \ge 7$. Die übrigen Fälle $k_1 = 2$ und $k_2 = 2, 3, 5$ wurden nachgerechnet.

    Nun zum Beweis von Teil 2. Aus (\ref{eq:lki-asymp}) folgt $L_{k, k}(p) = L_{k}(p) / \sqrt 2$ für $1 \le k \in \N$. Damit folgt Teil 2 des Satzes aus Satz \ref{theorem:optimal-k-m1}.
\end{proof}

\section{Experimentelle Ergebnisse}


\begin{figure}
    \centering
    \begin{tikzpicture}
        \begin{axis}[
                xlabel = {$k$},
                xmin = 1,
                xmax = 48,
                xtick = {1, 4, 8, 12, 16, 20, 24, 28, 32, 36, 40, 44, 48},
                width = \textwidth,
                height = 260pt,
                legend pos = north west,
                legend style = {draw = none},
                legend cell align = left,
            ]

            \addplot[color=magenta,mark=square]
            coordinates{
                    (1, 1.0)
                    (2, 1.5773502691896257)
                    (3, 2.1086517918741277)
                    (4, 2.21648605664914)
                    (5, 2.8790043489023804)
                    (6, 2.332272327437964)
                    (7, 3.414299970503978)
                    (8, 2.89543195056782)
                    (9, 3.3067332978837425)
                    (10, 2.970093877947532)
                    (11, 4.142494904167608)
                    (12, 2.7949482208102387)
                    (13, 4.411181889332409)
                    (14, 3.4111279805567123)
                    (15, 3.481018974295858)
                    (16, 3.594729442047154)
                    (17, 4.840295239186588)
                    (18, 3.2665648302249286)
                    (19, 5.017128905823505)
                    (20, 3.426648342312123)
                    (21, 3.952865384732847)
                    (22, 4.006284568144385)
                    (23, 5.319207262349245)
                    (24, 3.3342189545431045)
                    (25, 4.848341222073144)
                    (26, 4.224515025049569)
                    (27, 4.541184581320172)
                    (28, 3.8606214340939586)
                    (29, 5.682737117278753)
                    (30, 3.3580839014528188)
                    (31, 5.786717613861166)
                    (32, 4.303622115889641)
                    (33, 4.585058767874234)
                    (34, 4.57165798930078)
                    (35, 4.768544753994481)
                    (36, 3.65286546982577)
                    (37, 6.061272505622063)
                    (38, 4.714254318317245)
                    (39, 4.815669489895146)
                    (40, 3.9872807230736402)
                    (41, 6.219718898826906)
                    (42, 3.7480426730792065)
                    (43, 6.293026650584624)
                    (44, 4.440755450349039)
                    (45, 4.475806923989725)
                    (46, 4.957283888509379)
                    (47, 6.429590774020685)
                    (48, 3.903718627468377)
                };

            \addplot[color=blue,mark=square]
            coordinates {
                    (1, 1.0)
                    (2, 1.509948118652118)
                    (3, 2.0993697474766457)
                    (4, 2.1116345085043)
                    (5, 2.877418891483742)
                    (6, 2.5685036940000137)
                    (7, 2.996430188276255)
                    (8, 2.7314845124186458)
                    (9, 3.328296749381993)
                    (10, 3.2577944117129998)
                    (11, 3.7230055520415988)
                    (12, 3.1561945991451923)
                    (13, 3.736074156058803)
                    (14, 3.3685945128552692)
                    (15, 3.319276959371872)
                    (16, 3.3613292919244078)
                    (17, 4.394228617904989)
                    (18, 3.722443809049611)
                    (19, 4.451548228110135)
                    (20, 3.84047914786071)
                    (21, 3.920376539331896)
                    (22, 4.032026952636839)
                    (23, 4.529968590050905)
                    (24, 3.707477302822801)
                    (25, 4.242901333931946)
                    (26, 4.037016391081217)
                    (27, 4.058783598622834)
                    (28, 3.9310913363101916)
                    (29, 4.522555243332595)
                    (30, 3.7415823259420846)
                    (31, 4.59640961602802)
                    (32, 3.982932254189798)
                    (33, 4.510560919218005)
                    (34, 4.624088329155094)
                    (35, 4.744870700917283)
                    (36, 4.277919837533675)
                    (37, 5.181576815360558)
                    (38, 4.687283294435714)
                    (39, 4.642788634977157)
                    (40, 4.449130421846238)
                    (41, 5.186842863188695)
                    (42, 4.302098653348379)
                    (43, 5.24978349062113)
                    (44, 4.594828700423131)
                    (45, 4.4958880668775825)
                    (46, 4.73608695893801)
                    (47, 5.293961293662393)
                    (48, 4.28487474982348)};

            \legend{Formel (\ref{eq:lki-asymp}), Gemessene Laufzeit}
        \end{axis}
    \end{tikzpicture}
    \caption{Die durchschnittliche gemessene Laufzeit des Rho-Algorithmus für $M = 1$ und Werte von Formel (\ref{eq:lki-asymp}) für $1 \le k \le 48$.}
    \label{fig:measurements-m1}

    \vspace{2em}

    \small

    \begin{tabular}{c|cccccccccccccc}
        $k_1 \backslash k_2$ & 1                                & 2                                & 3                                & 4                                & 5                                & 6                                & 7                                & 8                                & 9                                & 10                               & 11                               & 12                               & 13                               & 14                               \\
        \hline
        1                    & \textcolor[HTML]{ 0020ff }{1.00} & \textcolor[HTML]{ 0320fb }{1.03} & \textcolor[HTML]{ 0720f7 }{1.08} & \textcolor[HTML]{ 0720f7 }{1.08} & \textcolor[HTML]{ 0820f6 }{1.09} & \textcolor[HTML]{ 0820f6 }{1.09} & \textcolor[HTML]{ 0820f6 }{1.09} & \textcolor[HTML]{ 0820f6 }{1.09} & \textcolor[HTML]{ 0820f6 }{1.09} & \textcolor[HTML]{ 0820f6 }{1.09} & \textcolor[HTML]{ 0820f6 }{1.09} & \textcolor[HTML]{ 0820f6 }{1.09} & \textcolor[HTML]{ 0820f6 }{1.09} & \textcolor[HTML]{ 0820f6 }{1.09} \\
        2                    &                                  & \textcolor[HTML]{ 3220cc }{1.55} & \textcolor[HTML]{ 3320cb }{1.55} & \textcolor[HTML]{ 3420ca }{1.57} & \textcolor[HTML]{ 3920c5 }{1.63} & \textcolor[HTML]{ 3920c5 }{1.62} & \textcolor[HTML]{ 3a20c4 }{1.63} & \textcolor[HTML]{ 3920c5 }{1.63} & \textcolor[HTML]{ 3b20c3 }{1.65} & \textcolor[HTML]{ 3c20c2 }{1.65} & \textcolor[HTML]{ 3c20c2 }{1.65} & \textcolor[HTML]{ 3b20c3 }{1.65} & \textcolor[HTML]{ 3c20c2 }{1.65} & \textcolor[HTML]{ 3d20c1 }{1.66} \\
        3                    &                                  &                                  & \textcolor[HTML]{ 6c2092 }{2.18} & \textcolor[HTML]{ 60209e }{2.05} & \textcolor[HTML]{ 6b2093 }{2.16} & \textcolor[HTML]{ 6a2094 }{2.15} & \textcolor[HTML]{ 6c2092 }{2.18} & \textcolor[HTML]{ 692095 }{2.15} & \textcolor[HTML]{ 72208c }{2.24} & \textcolor[HTML]{ 70208e }{2.21} & \textcolor[HTML]{ 72208c }{2.24} & \textcolor[HTML]{ 70208e }{2.22} & \textcolor[HTML]{ 72208c }{2.24} & \textcolor[HTML]{ 71208d }{2.22} \\
        4                    &                                  &                                  &                                  & \textcolor[HTML]{ 6d2091 }{2.18} & \textcolor[HTML]{ 6c2092 }{2.17} & \textcolor[HTML]{ 692095 }{2.15} & \textcolor[HTML]{ 72208c }{2.24} & \textcolor[HTML]{ 6d2091 }{2.19} & \textcolor[HTML]{ 71208d }{2.23} & \textcolor[HTML]{ 72208c }{2.24} & \textcolor[HTML]{ 73208b }{2.25} & \textcolor[HTML]{ 71208d }{2.23} & \textcolor[HTML]{ 752089 }{2.27} & \textcolor[HTML]{ 73208b }{2.25} \\
        5                    &                                  &                                  &                                  &                                  & \textcolor[HTML]{ b2204c }{2.94} & \textcolor[HTML]{ 972067 }{2.64} & \textcolor[HTML]{ a2205c }{2.76} & \textcolor[HTML]{ 9c2062 }{2.69} & \textcolor[HTML]{ a72057 }{2.81} & \textcolor[HTML]{ a92055 }{2.84} & \textcolor[HTML]{ af204f }{2.89} & \textcolor[HTML]{ a3205b }{2.77} & \textcolor[HTML]{ af204f }{2.90} & \textcolor[HTML]{ a92055 }{2.83} \\
        6                    &                                  &                                  &                                  &                                  &                                  & \textcolor[HTML]{ 9f205f }{2.72} & \textcolor[HTML]{ 992065 }{2.66} & \textcolor[HTML]{ 982066 }{2.65} & \textcolor[HTML]{ 9e2060 }{2.71} & \textcolor[HTML]{ 9c2062 }{2.69} & \textcolor[HTML]{ 9e2060 }{2.72} & \textcolor[HTML]{ 9e2060 }{2.71} & \textcolor[HTML]{ 9f205f }{2.72} & \textcolor[HTML]{ 9d2061 }{2.70} \\
        7                    &                                  &                                  &                                  &                                  &                                  &                                  & \textcolor[HTML]{ bb2043 }{3.03} & \textcolor[HTML]{ a1205d }{2.75} & \textcolor[HTML]{ af204f }{2.89} & \textcolor[HTML]{ ac2052 }{2.87} & \textcolor[HTML]{ b62048 }{2.97} & \textcolor[HTML]{ a92055 }{2.83} & \textcolor[HTML]{ b52049 }{2.97} & \textcolor[HTML]{ b1204d }{2.92} \\
        8                    &                                  &                                  &                                  &                                  &                                  &                                  &                                  & \textcolor[HTML]{ a92055 }{2.84} & \textcolor[HTML]{ a2205c }{2.76} & \textcolor[HTML]{ a4205a }{2.78} & \textcolor[HTML]{ a82056 }{2.82} & \textcolor[HTML]{ a2205c }{2.76} & \textcolor[HTML]{ a82056 }{2.82} & \textcolor[HTML]{ a62058 }{2.80} \\
        9                    &                                  &                                  &                                  &                                  &                                  &                                  &                                  &                                  & \textcolor[HTML]{ e3201b }{3.46} & \textcolor[HTML]{ cf202f }{3.24} & \textcolor[HTML]{ d72027 }{3.33} & \textcolor[HTML]{ d0202e }{3.25} & \textcolor[HTML]{ d72027 }{3.33} & \textcolor[HTML]{ d1202d }{3.26} \\
        10                   &                                  &                                  &                                  &                                  &                                  &                                  &                                  &                                  &                                  & \textcolor[HTML]{ df201f }{3.42} & \textcolor[HTML]{ d52029 }{3.31} & \textcolor[HTML]{ cd2031 }{3.22} & \textcolor[HTML]{ d52029 }{3.31} & \textcolor[HTML]{ d2202c }{3.28} \\
        11                   &                                  &                                  &                                  &                                  &                                  &                                  &                                  &                                  &                                  &                                  & \textcolor[HTML]{ fe2000 }{3.75} & \textcolor[HTML]{ d0202e }{3.26} & \textcolor[HTML]{ e62018 }{3.50} & \textcolor[HTML]{ db2023 }{3.38} \\
        12                   &                                  &                                  &                                  &                                  &                                  &                                  &                                  &                                  &                                  &                                  &                                  & \textcolor[HTML]{ d62028 }{3.32} & \textcolor[HTML]{ d0202e }{3.26} & \textcolor[HTML]{ cf202f }{3.24} \\
        13                   &                                  &                                  &                                  &                                  &                                  &                                  &                                  &                                  &                                  &                                  &                                  &                                  & \textcolor[HTML]{ ff2000 }{3.76} & \textcolor[HTML]{ db2023 }{3.38} \\
        14                   &                                  &                                  &                                  &                                  &                                  &                                  &                                  &                                  &                                  &                                  &                                  &                                  &                                  & \textcolor[HTML]{ e72017 }{3.51} \\
    \end{tabular}

    \vspace{0.2em}

    \begin{tabular}{c|cccccccccccccc}
        $k_1 \backslash k_2$ & 1                                & 2                                & 3                                & 4                                & 5                                & 6                                & 7                                & 8                                & 9                                & 10                               & 11                               & 12                               & 13                               & 14                               \\
        \hline
        1                    & \textcolor[HTML]{ 0020ff }{1.00} & \textcolor[HTML]{ 0420fa }{1.06} & \textcolor[HTML]{ 0820f6 }{1.12} & \textcolor[HTML]{ 0920f5 }{1.12} & \textcolor[HTML]{ 0c20f2 }{1.17} & \textcolor[HTML]{ 0920f5 }{1.13} & \textcolor[HTML]{ 0e20f0 }{1.19} & \textcolor[HTML]{ 0c20f2 }{1.16} & \textcolor[HTML]{ 0d20f1 }{1.18} & \textcolor[HTML]{ 0c20f2 }{1.17} & \textcolor[HTML]{ 1020ee }{1.22} & \textcolor[HTML]{ 0b20f3 }{1.15} & \textcolor[HTML]{ 1020ee }{1.22} & \textcolor[HTML]{ 0e20f0 }{1.19} \\
        2                    &                                  & \textcolor[HTML]{ 2b20d3 }{1.58} & \textcolor[HTML]{ 2620d8 }{1.51} & \textcolor[HTML]{ 2720d7 }{1.53} & \textcolor[HTML]{ 3120cd }{1.66} & \textcolor[HTML]{ 2820d6 }{1.55} & \textcolor[HTML]{ 3620c8 }{1.73} & \textcolor[HTML]{ 2f20cf }{1.64} & \textcolor[HTML]{ 3420ca }{1.70} & \textcolor[HTML]{ 3120cd }{1.66} & \textcolor[HTML]{ 3c20c2 }{1.81} & \textcolor[HTML]{ 2d20d1 }{1.61} & \textcolor[HTML]{ 3d20c1 }{1.83} & \textcolor[HTML]{ 3520c9 }{1.71} \\
        3                    &                                  &                                  & \textcolor[HTML]{ 5220ac }{2.11} & \textcolor[HTML]{ 3a20c4 }{1.78} & \textcolor[HTML]{ 4b20b3 }{2.01} & \textcolor[HTML]{ 3c20c2 }{1.81} & \textcolor[HTML]{ 5420aa }{2.14} & \textcolor[HTML]{ 4820b6 }{1.97} & \textcolor[HTML]{ 5020ae }{2.07} & \textcolor[HTML]{ 4a20b4 }{2.00} & \textcolor[HTML]{ 5e20a0 }{2.27} & \textcolor[HTML]{ 4420ba }{1.92} & \textcolor[HTML]{ 61209d }{2.31} & \textcolor[HTML]{ 5220ac }{2.10} \\
        4                    &                                  &                                  &                                  & \textcolor[HTML]{ 5a20a4 }{2.22} & \textcolor[HTML]{ 4e20b0 }{2.05} & \textcolor[HTML]{ 3f20bf }{1.85} & \textcolor[HTML]{ 5920a5 }{2.19} & \textcolor[HTML]{ 4b20b3 }{2.02} & \textcolor[HTML]{ 5420aa }{2.12} & \textcolor[HTML]{ 4d20b1 }{2.04} & \textcolor[HTML]{ 63209b }{2.34} & \textcolor[HTML]{ 4720b7 }{1.96} & \textcolor[HTML]{ 672097 }{2.38} & \textcolor[HTML]{ 5620a8 }{2.15} \\
        5                    &                                  &                                  &                                  &                                  & \textcolor[HTML]{ 8c2072 }{2.88} & \textcolor[HTML]{ 5220ac }{2.10} & \textcolor[HTML]{ 782086 }{2.61} & \textcolor[HTML]{ 64209a }{2.35} & \textcolor[HTML]{ 71208d }{2.51} & \textcolor[HTML]{ 672097 }{2.38} & \textcolor[HTML]{ 892075 }{2.83} & \textcolor[HTML]{ 5e20a0 }{2.27} & \textcolor[HTML]{ 8e2070 }{2.90} & \textcolor[HTML]{ 73208b }{2.55} \\
        6                    &                                  &                                  &                                  &                                  &                                  & \textcolor[HTML]{ 63209b }{2.33} & \textcolor[HTML]{ 5d20a1 }{2.25} & \textcolor[HTML]{ 4f20af }{2.06} & \textcolor[HTML]{ 5820a6 }{2.18} & \textcolor[HTML]{ 5120ad }{2.09} & \textcolor[HTML]{ 692095 }{2.41} & \textcolor[HTML]{ 4b20b3 }{2.00} & \textcolor[HTML]{ 6c2092 }{2.46} & \textcolor[HTML]{ 5a20a4 }{2.21} \\
        7                    &                                  &                                  &                                  &                                  &                                  &                                  & \textcolor[HTML]{ b4204a }{3.41} & \textcolor[HTML]{ 74208a }{2.56} & \textcolor[HTML]{ 83207b }{2.76} & \textcolor[HTML]{ 772087 }{2.60} & \textcolor[HTML]{ a1205d }{3.16} & \textcolor[HTML]{ 6d2091 }{2.46} & \textcolor[HTML]{ a82056 }{3.26} & \textcolor[HTML]{ 862078 }{2.81} \\
        8                    &                                  &                                  &                                  &                                  &                                  &                                  &                                  & \textcolor[HTML]{ 8d2071 }{2.90} & \textcolor[HTML]{ 6d2091 }{2.47} & \textcolor[HTML]{ 64209a }{2.34} & \textcolor[HTML]{ 852079 }{2.79} & \textcolor[HTML]{ 5c20a2 }{2.23} & \textcolor[HTML]{ 8a2074 }{2.86} & \textcolor[HTML]{ 70208e }{2.51} \\
        9                    &                                  &                                  &                                  &                                  &                                  &                                  &                                  &                                  & \textcolor[HTML]{ ac2052 }{3.31} & \textcolor[HTML]{ 70208e }{2.51} & \textcolor[HTML]{ 982066 }{3.04} & \textcolor[HTML]{ 672097 }{2.38} & \textcolor[HTML]{ 9f205f }{3.13} & \textcolor[HTML]{ 7f207f }{2.70} \\
        10                   &                                  &                                  &                                  &                                  &                                  &                                  &                                  &                                  &                                  & \textcolor[HTML]{ 93206b }{2.97} & \textcolor[HTML]{ 892075 }{2.83} & \textcolor[HTML]{ 5e20a0 }{2.26} & \textcolor[HTML]{ 8e2070 }{2.91} & \textcolor[HTML]{ 73208b }{2.55} \\
        11                   &                                  &                                  &                                  &                                  &                                  &                                  &                                  &                                  &                                  &                                  & \textcolor[HTML]{ ea2014 }{4.14} & \textcolor[HTML]{ 7d2081 }{2.68} & \textcolor[HTML]{ c72037 }{3.67} & \textcolor[HTML]{ 9c2062 }{3.09} \\
        12                   &                                  &                                  &                                  &                                  &                                  &                                  &                                  &                                  &                                  &                                  &                                  & \textcolor[HTML]{ 862078 }{2.79} & \textcolor[HTML]{ 82207c }{2.74} & \textcolor[HTML]{ 692095 }{2.42} \\
        13                   &                                  &                                  &                                  &                                  &                                  &                                  &                                  &                                  &                                  &                                  &                                  &                                  & \textcolor[HTML]{ ff2000 }{4.41} & \textcolor[HTML]{ a3205b }{3.18} \\
        14                   &                                  &                                  &                                  &                                  &                                  &                                  &                                  &                                  &                                  &                                  &                                  &                                  &                                  & \textcolor[HTML]{ b4204a }{3.41} \\
    \end{tabular}

    \caption{Die durchschnittliche Laufzeit von Pollards Rho-Algorithmus (oben) und berechnete Werte für die erwartete Laufzeit (unten), für $M = 2$ und $1 \le k_1 \le k_2 \le 14$. }
    \label{fig:measurements-m2}
\end{figure}

Um zu demonstrieren, dass Satz \ref{theorem:lki-asymp} das Laufzeitverhalten von Pollards Rho-Algorithmus gut beschreibt, wurden Laufzeitmessungen für $M = 1$ und $M = 2$ durchgeführt. Als Testzahlen wurden 192-Bit-Zahlen verwendet, bei denen jeder Primfaktor größer als $2^{24}$ ist. Die Größe der Primfaktoren nach unten zu beschränken ist sinnvoll, da die Laufzeit der Rho-Methode sonst sehr kurz wäre und die Messergebnisse durch Dinge wie den Aufwand der Laufzeitmessung selbst verfälscht werden würden. Die Ergebnisse und Werte von (\ref{eq:lki-asymp}) zum Vergleich sind in den Abbildungen \ref{fig:measurements-m1} und \ref{fig:measurements-m2} dargestellt. Sowohl die Werte der Formel als auch die Messdaten wurden skaliert, sodass bei $k = 1$ bzw. $k_1 = k_2 = 1$ der Wert 1 steht.

\emph{Eine Maschine.} Die Laufzeit wird gut von (\ref{eq:lki-asymp}) angenähert, denn die Werte befinden sich in der gleichen Größenordnung und auch Charakteristika spezieller Zahlen, wie beispielsweise hohe Werte bei Primzahlen, werden von beiden reflektiert.

\emph{Zwei Maschinen.} Auch hier wird das grundsätzliche Verhalten der Laufzeit gut durch (\ref{eq:lki-asymp}) beschreiben. Die Aussage von Satz \ref{theorem:optimal-k-m2} wird bestätigt, und es gibt zumindest für $1 \le k_1 \le k_2 \le 14$ kein Paar $k_1, k_2$ mit einer geringeren Laufzeit als $k_1 = k_2 = 1$. Da die Laufzeit für größere $k_1, k_2$ tendenziell zu steigen scheint, wird die Vermutung aufgestellt, dass $k_1 = k_2 = 1$ optimal ist. Auffällig ist, dass die Laufzeitunterschiede weniger ausgeprägt sind als von den Formeln vorhergesagt. Das liegt wahrscheinlich daran, dass neben der Berechnung von $x^{2k}$ auch andere Berechnungen im Rho-Algorithmus durchgeführt werden, deren Dauer unabhängig von $k$ ist (z.B. das Bilden des $\gcd$). Diese werden aber in den Formeln nicht berücksichtigt.

\section{Fazit}

Zur Beantwortung der Frage nach der optimalen Wahl des Parameters $k$ konnten in dieser Arbeit grundlegende Formeln und Methoden entwickelt werden. Unter üblichen Annahmen über Pollards Rho-Algorithmus wurde eine Formel für die erwartete Laufzeit in Abhängigkeit der $k_i \ (1 \le i \le M)$ aufgestellt. Die damit erzielten Ergebnisse für $M = 1$ und $M = 2$ werden von Laufzeitmessungen unterstützt.

Für eine vollständige Beantwortung der Frage sind allerdings noch weitere Schritte nötig. Der Fall $M = 2$ müsste vollständig geklärt werden, und $M \ge 3$ wurde in dieser Arbeit noch nicht behandelt. Darüber hinaus stellt sich die Frage, ob es sinnvoll ist, nach der Wahl von $k$-Werten während der gesamten Ausführung des Algorithmus bei diesen zu bleiben. Möglicherweise ist es besser, dass eine Maschine nach längerer Zeit mit einem bestimmten Wert von $k$ diesen aufgibt und mit einem anderen $k$ weitermacht.

\newpage
\printbibliography

\end{document}